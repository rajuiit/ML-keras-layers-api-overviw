{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different activation functions are used for different purposes in neural networks. Here's a brief overview:\n",
    "\n",
    "1. **ReLU (Rectified Linear Activation):**\n",
    "   - Most commonly used activation function.\n",
    "   - Used in hidden layers to introduce non-linearity.\n",
    "   - Helps in overcoming the vanishing gradient problem.\n",
    "   - Defined as: \\( f(x) = \\max(0, x) \\)\n",
    "\n",
    "2. **Sigmoid:**\n",
    "   - Used in the output layer of binary classification models.\n",
    "   - Squeezes the output values between 0 and 1, interpreting them as probabilities.\n",
    "   - Defined as: \\( f(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "\n",
    "3. **Softmax:**\n",
    "   - Used in the output layer of multi-class classification models.\n",
    "   - Normalizes the output values into a probability distribution over multiple classes, ensuring that the sum of probabilities is 1.\n",
    "   - Defined as: \\( f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\)\n",
    "\n",
    "4. **Tanh (Hyperbolic Tangent):**\n",
    "   - Similar to sigmoid but with output values ranging from -1 to 1.\n",
    "   - Used in hidden layers of neural networks.\n",
    "   - Defined as: \\( f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \\)\n",
    "\n",
    "5. **Linear (or Identity):**\n",
    "   - Used in regression tasks where the output needs to be a continuous value.\n",
    "   - Simply passes the input through unchanged.\n",
    "   - Defined as: \\( f(x) = x \\)\n",
    "\n",
    "6. **Leaky ReLU:**\n",
    "   - Variation of ReLU that allows a small, non-zero gradient when the input is negative.\n",
    "   - Helps address the dying ReLU problem.\n",
    "   - Defined as: \\( f(x) = \\max(\\alpha x, x) \\), where \\(\\alpha\\) is a small constant.\n",
    "\n",
    "7. **ELU (Exponential Linear Unit):**\n",
    "   - Similar to ReLU but allows negative values with a smooth transition.\n",
    "   - May help learning in deeper networks.\n",
    "   - Defined as: \\( f(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ \\alpha (e^x - 1) & \\text{if } x < 0 \\end{cases} \\), where \\(\\alpha\\) is a small constant.\n",
    "\n",
    "These are some of the commonly used activation functions, each serving specific purposes in different parts of a neural network. Choosing the appropriate activation function depends on the nature of the problem and the properties of the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
