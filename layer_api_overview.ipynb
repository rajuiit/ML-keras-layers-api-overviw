{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Normalization layers are used in neural networks to standardize the inputs to a layer, which can improve the training process and overall performance of the model. Here are some key reasons why normalization layers are used:\n",
    "\n",
    "### 1. **Improving Convergence Speed**\n",
    "\n",
    "Normalization can help accelerate the training of deep neural networks by ensuring that the inputs to each layer have a consistent scale. This prevents the model from getting stuck in certain regions of the loss landscape, which can slow down convergence.\n",
    "\n",
    "### 2. **Reducing Internal Covariate Shift**\n",
    "\n",
    "Internal covariate shift refers to the changes in the distribution of layer inputs during training, which can make it harder for the network to learn. Normalization helps mitigate this problem by maintaining the mean and variance of the inputs to each layer, leading to more stable and faster training.\n",
    "\n",
    "### 3. **Reducing Overfitting**\n",
    "\n",
    "Normalization layers can have a regularizing effect, which helps reduce overfitting. By scaling the inputs, the network becomes less sensitive to the specific scale of the inputs, which can act as a form of regularization.\n",
    "\n",
    "### 4. **Improving Gradient Flow**\n",
    "\n",
    "By normalizing the inputs to each layer, the gradients during backpropagation can flow more smoothly through the network. This helps avoid problems like vanishing or exploding gradients, which are common in deep networks.\n",
    "\n",
    "### 5. **Enabling Higher Learning Rates**\n",
    "\n",
    "Normalization allows for the use of higher learning rates, which can speed up the training process. Without normalization, high learning rates might cause the model to diverge.\n",
    "\n",
    "### Common Types of Normalization Layers\n",
    "\n",
    "#### 1. **Batch Normalization**\n",
    "\n",
    "- **How it works**: Batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "- **Usage**: Typically used after a convolutional layer or a dense layer.\n",
    "  \n",
    "  ```python\n",
    "  from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "  model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "  model.add(BatchNormalization())\n",
    "  ```\n",
    "\n",
    "#### 2. **Layer Normalization**\n",
    "\n",
    "- **How it works**: Layer normalization normalizes the inputs across the features for each training example independently.\n",
    "- **Usage**: Often used in recurrent neural networks (RNNs) and transformers.\n",
    "  \n",
    "  ```python\n",
    "  from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "  model.add(layers.Dense(128, activation='relu'))\n",
    "  model.add(LayerNormalization())\n",
    "  ```\n",
    "\n",
    "#### 3. **Instance Normalization**\n",
    "\n",
    "- **How it works**: Instance normalization normalizes the inputs for each sample in a batch independently.\n",
    "- **Usage**: Commonly used in style transfer networks and other tasks where per-instance normalization is beneficial.\n",
    "\n",
    "  ```python\n",
    "  from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "  model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "  model.add(InstanceNormalization())\n",
    "  ```\n",
    "\n",
    "#### 4. **Group Normalization**\n",
    "\n",
    "- **How it works**: Group normalization divides the channels into groups and normalizes each group independently.\n",
    "- **Usage**: Useful for small batch sizes where batch normalization might not be effective.\n",
    "  \n",
    "  ```python\n",
    "  from tensorflow_addons.layers import GroupNormalization\n",
    "\n",
    "  model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "  model.add(GroupNormalization(groups=4))\n",
    "  ```\n",
    "\n",
    "### Example of Using Normalization Layers\n",
    "\n",
    "Here is an example of incorporating batch normalization into a convolutional neural network (CNN) using the Sequential API:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "In this example, `BatchNormalization` layers are added after each convolutional layer and dense layer to normalize their outputs. This helps in stabilizing and speeding up the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "### Introduction:\n",
    "Regularization techniques are essential for preventing overfitting and improving the generalization performance of machine learning models. In this tutorial, we'll explore different regularization techniques available in the Keras library and learn how to implement them effectively.\n",
    "\n",
    "Tutorial Link 1: https://towardsdatascience.com/everything-you-need-to-know-about-regularization-64734f240622\n",
    "\n",
    "Tutorial Link 2: https://medium.com/@francescofranco_39234/dropout-regularization-with-keras-7b89651da252"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
